# -*- coding: utf-8 -*-
"""AlphabetSoupCharity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/138F3mA7gX0465V4f-TT40QvHrraUYCP3

### Deliverable 1: Preprocessing the Data for a Neural Network
"""

# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,OneHotEncoder
import pandas as pd
import tensorflow as tf


import pandas as pd

#import csv to Colab
url = "https://raw.githubusercontent.com/camillecoding/Neural_Network_Charity_Analysis/main/charity_data.csv"

#  Import and read the charity_data.csv.
application_df = pd.read_csv(url)
application_df.head()

# Drop the non-beneficial ID columns, 'EIN' and 'NAME'. - NOTE performance will be determined by "IS_SUCCESSFUL"
application_df = application_df.drop(columns=["EIN", "NAME"])
application_df.head()

# Determine the number of unique values in each column.
application_df.nunique()

# Look at APPLICATION_TYPE value counts for binning
values = application_df.APPLICATION_TYPE.value_counts()
values

# Visualize the value counts of APPLICATION_TYPE
values.plot.density()

# Determine which values to replace if counts are less than 500
low_density = list(values[values < 500].index)

# Replace in dataframe
for app in low_density:
    application_df.APPLICATION_TYPE = application_df.APPLICATION_TYPE.replace(app,"Other")
    
# Check to make sure binning was successful
application_df.APPLICATION_TYPE.value_counts()

# Look at CLASSIFICATION value counts for binning
classification = application_df.CLASSIFICATION.value_counts()
classification

# Visualize the value counts of CLASSIFICATION
classification.plot.density()

# Determine which values to replace if counts are less than 1883
replace_class = list(classification[classification < 1880].index)

# Replace in dataframe
for cls in replace_class:
    application_df.CLASSIFICATION = application_df.CLASSIFICATION.replace(cls,"Other")
    
# Check to make sure binning was successful
application_df.CLASSIFICATION.value_counts()

# Generate our categorical variable lists
cat_app = application_df.dtypes[application_df.dtypes == 'object'].index.to_list()

# Create a OneHotEncoder instance
enc = OneHotEncoder(sparse=False)

# Fit and transform the OneHotEncoder using the categorical variable list
encode_df = pd.DataFrame(enc.fit_transform(application_df[cat_app]))

# Add the encoded variable names to the dataframe
encode_df.columns = enc.get_feature_names(cat_app)
encode_df.head()

# Merge one-hot encoded features and drop the originals
application_df = application_df.merge(encode_df, left_index=True, right_index=True).drop(cat_app, 1)
application_df.head()

# Split our preprocessed data into our features and target arrays
y = application_df["IS_SUCCESSFUL"].values
X = application_df.drop(["IS_SUCCESSFUL"])

# Split the preprocessed data into a training and testing dataset
X_train, y_train, X_test, y_test = train_test_split(X, y, random_state=78)

# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

"""### Deliverable 2: Compile, Train and Evaluate the Model"""

# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE

nn = tf.keras.models.Sequential()

# First hidden layer
#  YOUR CODE GOES HERE

# Second hidden layer
#  YOUR CODE GOES HERE

# Output layer
#  YOUR CODE GOES HERE

# Check the structure of the model
nn.summary()

# Compile the model
#  YOUR CODE GOES HERE

# Train the model
#  YOUR CODE GOES HERE

# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

